In building my pipeline, my first step was to analyze the data distribution. The numerical features varied significantly in scale, so I applied scaling to normalize them and prevent models from being biased toward large-magnitude features. While I identified many outliers, I chose not to remove them because doing so would have led to a significant loss of data and possibly distorted the real distribution. For categorical features, I applied one-hot encoding to ensure that models could properly interpret non-numeric values.
One particularly challenging column contained multiple file extensions within a single entry. To handle this, I processed it into a multi-hot encoded representation, where each unique file extension became a separate column with binary indicators (0/1) specifying its presence. This transformation allowed the model to leverage file extension patterns without losing information. During exploratory data analysis, I found that almost every categorical feature contributed meaningfully to the target label (role classification). Hence, I retained and used all categorical columns in the pipeline.
For the text column, I employed TF-IDF vectorization as a baseline approach. This method is widely proven to capture word importance effectively and provided a solid foundation for applying traditional ML models such as logistic regression, random forests, and gradient boosting. However, to capture deeper contextual information, I moved beyond classical ML and incorporated deep learning. I used a BiLSTM architecture, which processes text sequences in both directions and extracts richer semantic patterns.
Finally, I concatenated the BiLSTM text embeddings with the scaled numerical features and encoded categorical embeddings to create a unified feature representation. This design allowed the model to leverage signals from all data types jointly. The combination of structured feature engineering and contextual text modeling was the most important design choice, enabling stronger overall performance.